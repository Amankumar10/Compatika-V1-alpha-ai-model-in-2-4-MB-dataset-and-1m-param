{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f90479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train.csv: 76673it [00:00, 84456.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 76600 examples to data/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing valid.csv: 12030it [00:00, 86447.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 12019 examples to data/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test.csv: 10943it [00:00, 81132.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10932 examples to data/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# scripts/rebuild_from_csv.py\n",
    "import csv, json, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "def csv_to_jsonl(csv_path, out_path, user_col=\"context\", reply_col=\"utterance\", max_reply_words=80):\n",
    "    written = 0\n",
    "    with open(csv_path, newline='', encoding='utf-8') as f_in, open(out_path, 'w', encoding='utf-8') as f_out:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        for row in tqdm(reader, desc=f\"Processing {os.path.basename(csv_path)}\"):\n",
    "            user = row.get(user_col, \"\").strip()\n",
    "            reply = row.get(reply_col, \"\").strip()\n",
    "            if not user or not reply:\n",
    "                continue\n",
    "            # basic normalization of whitespace\n",
    "            user = \" \".join(user.split())\n",
    "            reply = \" \".join(reply.split())\n",
    "            # filter extremely long replies (optional)\n",
    "            if len(reply.split()) > max_reply_words:\n",
    "                continue\n",
    "            j = {\"user\": user, \"compatika\": reply}\n",
    "            f_out.write(json.dumps(j, ensure_ascii=False) + \"\\n\")\n",
    "            written += 1\n",
    "    print(f\"Saved {written} examples to {out_path}\")\n",
    "\n",
    "# adjust paths to your CSV files\n",
    "csv_to_jsonl(\"datas/empathetic_dialogues/train.csv\", \"data/train.jsonl\")\n",
    "csv_to_jsonl(\"datas/empathetic_dialogues/valid.csv\", \"data/val.jsonl\")\n",
    "csv_to_jsonl(\"datas/empathetic_dialogues/test.csv\", \"data/test.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa93b0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76600it [00:02, 37178.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 76600 cleaned examples to data/cleaned/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12019it [00:00, 37222.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 12019 cleaned examples to data/cleaned/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10932it [00:00, 35804.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10932 cleaned examples to data/cleaned/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# scripts/clean_jsonl.py\n",
    "import json, os, re\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs(\"data/cleaned\", exist_ok=True)\n",
    "\n",
    "REPLACEMENTS = [\n",
    "    (r\"_comma_\", \",\"),\n",
    "    (r\"_apostrophe_\", \"'\"),\n",
    "    (r\"_quote_\", '\"'),\n",
    "    (r\" +,\", \",\"),        # spaces before commas\n",
    "    (r\"\\s+([?.!,])\", r\"\\1\"), # fix spacing before punctuation\n",
    "]\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.strip()\n",
    "    for pat, repl in REPLACEMENTS:\n",
    "        s = re.sub(pat, repl, s)\n",
    "    # fix multiple spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    # fix spacing around parentheses and stray underscores\n",
    "    s = s.replace(\" _comma_\", \",\").replace(\"_\", \"\")\n",
    "    return s.strip()\n",
    "\n",
    "def clean_file(infile, outfile, max_reply_words=80):\n",
    "    kept = 0\n",
    "    with open(infile, 'r', encoding='utf-8') as fi, open(outfile, 'w', encoding='utf-8') as fo:\n",
    "        for line in tqdm(fi):\n",
    "            try:\n",
    "                j = json.loads(line)\n",
    "                user = clean_text(j.get(\"user\",\"\"))\n",
    "                reply = clean_text(j.get(\"compatika\",\"\"))\n",
    "                if not user or not reply:\n",
    "                    continue\n",
    "                # enforce reply length limit\n",
    "                if len(reply.split()) > max_reply_words:\n",
    "                    # shorten by taking first 40 words + \"...\"\n",
    "                    reply = \" \".join(reply.split()[:40]).strip() + \"...\"\n",
    "                fo.write(json.dumps({\"user\": user, \"compatika\": reply}, ensure_ascii=False) + \"\\n\")\n",
    "                kept += 1\n",
    "            except Exception:\n",
    "                continue\n",
    "    print(f\"Saved {kept} cleaned examples to {outfile}\")\n",
    "\n",
    "# clean all rebuilt files\n",
    "clean_file(\"data/train.jsonl\", \"data/cleaned/train.jsonl\")\n",
    "clean_file(\"data/val.jsonl\", \"data/cleaned/val.jsonl\")\n",
    "clean_file(\"data/test.jsonl\", \"data/cleaned/test.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d50f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76600it [00:00, 248655.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deduped count: 76066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# scripts/dedupe.py\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "seen = set()\n",
    "outf = open(\"data/cleaned/train_dedup.jsonl\",\"w\",encoding=\"utf-8\")\n",
    "count=0\n",
    "with open(\"data/cleaned/train.jsonl\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        j = json.loads(line)\n",
    "        key = (j[\"user\"], j[\"compatika\"])\n",
    "        if key in seen: continue\n",
    "        seen.add(key)\n",
    "        outf.write(line)\n",
    "        count+=1\n",
    "outf.close()\n",
    "print(\"deduped count:\", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7546cad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- preview data/cleaned/train.jsonl ----\n",
      "Sample 1:\n",
      " USER: sentimental\n",
      " COMPATIKA: I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\n",
      "\n",
      "Sample 2:\n",
      " USER: sentimental\n",
      " COMPATIKA: Was this a friend you were in love with, or just a best friend?\n",
      "\n",
      "Sample 3:\n",
      " USER: sentimental\n",
      " COMPATIKA: This was a best friend. I miss her.\n",
      "\n",
      "Sample 4:\n",
      " USER: sentimental\n",
      " COMPATIKA: Where has she gone?\n",
      "\n",
      "Sample 5:\n",
      " USER: sentimental\n",
      " COMPATIKA: We no longer talk.\n",
      "\n",
      "---- preview data/cleaned/val.jsonl ----\n",
      "Sample 1:\n",
      " USER: terrified\n",
      " COMPATIKA: Today,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\n",
      "\n",
      "Sample 2:\n",
      " USER: terrified\n",
      " COMPATIKA: Are you fine now?\n",
      "\n",
      "Sample 3:\n",
      " USER: terrified\n",
      " COMPATIKA: Yeah,i'm doing alright now, but with minor injuries.\n",
      "\n",
      "Sample 4:\n",
      " USER: terrified\n",
      " COMPATIKA: Cool :) Is your car damaged a lot?\n",
      "\n",
      "Sample 5:\n",
      " USER: terrified\n",
      " COMPATIKA: The car was badly damaged,i veered outside the road and hit a tree trunk. next thing is insurance follow up.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for i, path in enumerate([\"data/cleaned/train.jsonl\",\"data/cleaned/val.jsonl\"]):\n",
    "    print(\"---- preview\", path, \"----\")\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        for n,line in enumerate(f):\n",
    "            if n>=5: break\n",
    "            j=json.loads(line)\n",
    "            print(f\"Sample {n+1}:\\n USER: {j['user']}\\n COMPATIKA: {j['compatika']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c911ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags']\n",
      "First row example: ['hit:0_conv:1', '1', 'sentimental', 'I remember going to the fireworks with my best friend. There was a lot of people_comma_ but it only felt like us in the world.', '1', 'I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people_comma_ we felt like the only people in the world.', '5|5|5_2|2|5', '']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"datas/empathetic_dialogues/train.csv\", newline='', encoding='utf-8') as f:\n",
    "    r = csv.reader(f)\n",
    "    headers = next(r)\n",
    "    print(\"Headers:\", headers)\n",
    "    first = next(r)\n",
    "    print(\"First row example:\", first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dad6ca3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train.csv: 76673it [00:00, 89291.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 76600 examples to data/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing valid.csv: 12030it [00:00, 59126.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 12019 examples to data/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test.csv: 10943it [00:00, 81206.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 10932 examples to data/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv, json, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "def csv_to_jsonl(csv_path, out_path, user_col=\"prompt\", reply_col=\"utterance\", max_reply_words=80):\n",
    "    written = 0\n",
    "    with open(csv_path, newline='', encoding='utf-8') as f_in, open(out_path, 'w', encoding='utf-8') as f_out:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        for row in tqdm(reader, desc=f\"Processing {os.path.basename(csv_path)}\"):\n",
    "            user = row.get(user_col, \"\").strip()\n",
    "            reply = row.get(reply_col, \"\").strip()\n",
    "            if not user or not reply:\n",
    "                continue\n",
    "            # remove _comma_ etc\n",
    "            user = user.replace(\"_comma_\", \",\")\n",
    "            reply = reply.replace(\"_comma_\", \",\")\n",
    "            if len(reply.split()) > max_reply_words:\n",
    "                continue\n",
    "            j = {\"user\": user, \"compatika\": reply}\n",
    "            f_out.write(json.dumps(j, ensure_ascii=False) + \"\\n\")\n",
    "            written += 1\n",
    "    print(f\"‚úÖ Saved {written} examples to {out_path}\")\n",
    "\n",
    "# update paths as needed\n",
    "csv_to_jsonl(\"datas/empathetic_dialogues/train.csv\", \"data/train.jsonl\")\n",
    "csv_to_jsonl(\"datas/empathetic_dialogues/valid.csv\", \"data/val.jsonl\")\n",
    "csv_to_jsonl(\"datas/empathetic_dialogues/test.csv\", \"data/test.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d17e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      " USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      " COMPATIKA: I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\n",
      "\n",
      "Sample 2:\n",
      " USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      " COMPATIKA: Was this a friend you were in love with, or just a best friend?\n",
      "\n",
      "Sample 3:\n",
      " USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      " COMPATIKA: This was a best friend. I miss her.\n",
      "\n",
      "Sample 4:\n",
      " USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      " COMPATIKA: Where has she gone?\n",
      "\n",
      "Sample 5:\n",
      " USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      " COMPATIKA: We no longer talk.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        j = json.loads(line)\n",
    "        print(f\"Sample {i+1}:\\n USER: {j['user']}\\n COMPATIKA: {j['compatika']}\\n\")\n",
    "        if i >= 4:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67583d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Reading data/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train.jsonl: 76600it [00:01, 48746.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Reading data/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val.jsonl: 12019it [00:00, 46605.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Removing duplicates...\n",
      "üßæ Writing 88357 unique lines to data/text_for_tok.txt\n",
      "‚úÖ Done! Your tokenizer training file is ready at: data/text_for_tok.txt\n"
     ]
    }
   ],
   "source": [
    "# scripts/make_text_for_tokenizer.py\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Input and output paths\n",
    "input_files = [\n",
    "    \"data/train.jsonl\",\n",
    "    \"data/val.jsonl\"\n",
    "]\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "output_path = \"data/text_for_tok.txt\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic cleaning and normalization for tokenizer data\"\"\"\n",
    "    text = text.strip()\n",
    "    text = text.replace(\"_comma_\", \",\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s([?.!,])\", r\"\\1\", text)\n",
    "    text = text.replace(\" ,\", \",\")\n",
    "    text = text.replace(\" .\", \".\")\n",
    "    return text.strip()\n",
    "\n",
    "lines = []\n",
    "for path in input_files:\n",
    "    print(f\"üìñ Reading {path}\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=f\"Processing {os.path.basename(path)}\"):\n",
    "            try:\n",
    "                j = json.loads(line)\n",
    "                user = clean_text(j.get(\"user\", \"\"))\n",
    "                compatika = clean_text(j.get(\"compatika\", \"\"))\n",
    "                if not user or not compatika:\n",
    "                    continue\n",
    "                # Merge user + compatika into one line\n",
    "                merged = f\"{user} {compatika}\"\n",
    "                lines.append(merged)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "# Remove duplicates\n",
    "print(f\"üßπ Removing duplicates...\")\n",
    "unique_lines = list(set(lines))\n",
    "print(f\"üßæ Writing {len(unique_lines)} unique lines to {output_path}\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "    for line in unique_lines:\n",
    "        out.write(line.strip() + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Done! Your tokenizer training file is ready at:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08540f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I broked something at home and everyonw blamed my brother. I didn't say anything. I feel bad about it. That is terrible! Why did you blame him?\n",
      "i like person who are honestly with me That's a good way of saying live a good life.\n",
      "I stole from my parents as a kid and got caught. I felt so bad afterwards. I would feel guilty too if I was in your shoes. Did you feel bad because you stole or bad because you got caught?\n",
      "Doing a race with the wrong shoes. Good thing you found someone who had as big of feet as you do!\n",
      "I am so scary that my manager doesn't keep his promise for my promotion. No, no reason to he will do it.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/text_for_tok.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33ec1ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Reading data/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train.jsonl: 76600it [00:01, 51290.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Reading data/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val.jsonl: 12019it [00:00, 52296.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßæ Writing 102044 unique lines to data/text_for_tok.txt\n",
      "‚úÖ Clean tokenizer file ready: data/text_for_tok.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# scripts/make_text_for_tokenizer.py (clean single-line version)\n",
    "import json, os, re\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "input_files = [\"data/train.jsonl\", \"data/val.jsonl\"]\n",
    "output_path = \"data/text_for_tok.txt\"\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"_comma_\", \",\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s([?.!,])\", r\"\\1\", text)\n",
    "    text = text.replace(\" ,\", \",\").replace(\" .\", \".\")\n",
    "    return text.strip()\n",
    "\n",
    "lines = set()\n",
    "for path in input_files:\n",
    "    print(f\"üìñ Reading {path}\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=f\"Processing {os.path.basename(path)}\"):\n",
    "            try:\n",
    "                j = json.loads(line)\n",
    "                user = clean_text(j.get(\"user\", \"\"))\n",
    "                compatika = clean_text(j.get(\"compatika\", \"\"))\n",
    "                if not user or not compatika:\n",
    "                    continue\n",
    "                # add each sentence separately (not merged)\n",
    "                lines.add(user)\n",
    "                lines.add(compatika)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "print(f\"üßæ Writing {len(lines)} unique lines to {output_path}\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Clean tokenizer file ready:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad67b6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want them to be able to go to the store and buy everything with no worries.\n",
      "I was worried driving home tonight. People were swerving all over\n",
      "I had an interview for a hospital I want to volunteer at but I felt a little under-qualified\n",
      "Sorry about caps lock i didn't realize. And yea i think i'm going to propose here shortly.\n",
      "I was sad when my cat died. He was so sick\n",
      "The barber shop is totally worth it. They do such a good job.\n",
      "Wow what kind of toys did you find\n",
      "i was pissed when i saw someone left my gate open\n",
      "Got seasons pass to an amusement park.\n",
      "Yeah, that sounds like a really good idea!\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/text_for_tok.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ef349aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking data/train.jsonl: 76600it [00:00, 260384.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Checked 76600 lines. Issues found: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking data/val.jsonl: 12019it [00:00, 207128.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Checked 12019 lines. Issues found: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def verify_jsonl(path):\n",
    "    issues = 0\n",
    "    total = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(tqdm(f, desc=f\"Checking {path}\")):\n",
    "            total += 1\n",
    "            try:\n",
    "                j = json.loads(line)\n",
    "                if not isinstance(j.get(\"user\"), str) or not isinstance(j.get(\"compatika\"), str):\n",
    "                    print(f\"‚ùå Line {i+1}: non-string fields\")\n",
    "                    issues += 1\n",
    "                if not j[\"user\"].strip() or not j[\"compatika\"].strip():\n",
    "                    print(f\"‚ö†Ô∏è Line {i+1}: empty text\")\n",
    "                    issues += 1\n",
    "                if \"_comma_\" in j[\"user\"] or \"_comma_\" in j[\"compatika\"]:\n",
    "                    print(f\"‚ö†Ô∏è Line {i+1}: found _comma_ artifact\")\n",
    "                    issues += 1\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Line {i+1}: invalid JSON ({e})\")\n",
    "                issues += 1\n",
    "    print(f\"\\n‚úÖ Checked {total} lines. Issues found: {issues}\")\n",
    "\n",
    "verify_jsonl(\"data/train.jsonl\")\n",
    "verify_jsonl(\"data/val.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edfce3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      "COMPATIKA: I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\n",
      "\n",
      "USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      "COMPATIKA: Was this a friend you were in love with, or just a best friend?\n",
      "\n",
      "USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      "COMPATIKA: This was a best friend. I miss her.\n",
      "\n",
      "USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      "COMPATIKA: Where has she gone?\n",
      "\n",
      "USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      "COMPATIKA: We no longer talk.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"data/train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        j = json.loads(line)\n",
    "        print(f\"USER: {j['user']}\")\n",
    "        print(f\"COMPATIKA: {j['compatika']}\\n\")\n",
    "        if i >= 4: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec2e07ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vocab size: 10000\n",
      "Encoded IDs: [5, 87, 9, 364, 112, 204, 4]\n",
      "Decoded text: I feel a bit anxious today.\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"tok/compatika_sp.model\")\n",
    "\n",
    "print(\"‚úÖ Vocab size:\", sp.GetPieceSize())\n",
    "sample = \"I feel a bit anxious today.\"\n",
    "ids = sp.EncodeAsIds(sample)\n",
    "print(\"Encoded IDs:\", ids)\n",
    "print(\"Decoded text:\", sp.DecodeIds(ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5349e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want them to be able to go to the store and buy everything with no worries.\n",
      "I was worried driving home tonight. People were swerving all over\n",
      "I had an interview for a hospital I want to volunteer at but I felt a little under-qualified\n",
      "Sorry about caps lock i didn't realize. And yea i think i'm going to propose here shortly.\n",
      "I was sad when my cat died. He was so sick\n",
      "The barber shop is totally worth it. They do such a good job.\n",
      "Wow what kind of toys did you find\n",
      "i was pissed when i saw someone left my gate open\n",
      "Got seasons pass to an amusement park.\n",
      "Yeah, that sounds like a really good idea!\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/text_for_tok.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22d6264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking data/val.jsonl: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking data/val.jsonl: 12019it [00:00, 200608.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Checked 12019 lines. Issues: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def verify_jsonl(path):\n",
    "    total = 0\n",
    "    errors = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(tqdm(f, desc=f\"Checking {path}\")):\n",
    "            total += 1\n",
    "            try:\n",
    "                j = json.loads(line)\n",
    "                if not isinstance(j.get(\"user\"), str) or not isinstance(j.get(\"compatika\"), str):\n",
    "                    print(f\"‚ùå Line {i+1}: missing or invalid fields\")\n",
    "                    errors += 1\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Line {i+1}: invalid JSON ({e})\")\n",
    "                errors += 1\n",
    "    print(f\"\\n‚úÖ Checked {total} lines. Issues: {errors}\")\n",
    "\n",
    "verify_jsonl(\"data/val.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dffab28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want them to be able to go to the store and buy everything with no worries.\n",
      "I was worried driving home tonight. People were swerving all over\n",
      "I had an interview for a hospital I want to volunteer at but I felt a little under-qualified\n",
      "Sorry about caps lock i didn't realize. And yea i think i'm going to propose here shortly.\n",
      "I was sad when my cat died. He was so sick\n",
      "The barber shop is totally worth it. They do such a good job.\n",
      "Wow what kind of toys did you find\n",
      "i was pissed when i saw someone left my gate open\n",
      "Got seasons pass to an amusement park.\n",
      "Yeah, that sounds like a really good idea!\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/text_for_tok.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df8f6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vocab size: 10000\n",
      "Sample tokens: ['<pad>', '<unk>', '<s>', '</s>', '.', '‚ñÅI', '_', '‚ñÅto', \"'\", '‚ñÅa']\n",
      "Encode/Decode test:\n",
      "Encoded: [5, 87, 9, 364, 112, 204, 4]\n",
      "Decoded: I feel a bit anxious today.\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=\"tok/compatika_sp.model\")\n",
    "\n",
    "print(\"‚úÖ Vocab size:\", sp.get_piece_size())\n",
    "print(\"Sample tokens:\", [sp.id_to_piece(i) for i in range(10)])\n",
    "print(\"Encode/Decode test:\")\n",
    "ids = sp.encode(\"I feel a bit anxious today.\", out_type=int)\n",
    "print(\"Encoded:\", ids)\n",
    "print(\"Decoded:\", sp.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a678be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick example cleaning step\n",
    "import re, json\n",
    "def clean_text(t):\n",
    "    t = t.replace(\"_comma\", \",\")\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = re.sub(r\"[0-9]+\\|[0-9]+\\|[0-9]+[_0-9|]*\", \"\", t)  # remove rating patterns\n",
    "    t = re.sub(r\":\\d+\", \"\", t)  # remove stray numeric tokens\n",
    "    return t.strip()\n",
    "\n",
    "with open(\"data/train.jsonl\",\"r\",encoding=\"utf-8\") as f, open(\"data/train_clean.jsonl\",\"w\",encoding=\"utf-8\") as out:\n",
    "    for line in f:\n",
    "        s = json.loads(line)\n",
    "        s[\"user\"] = clean_text(s[\"user\"])\n",
    "        s[\"compatika\"] = clean_text(s[\"compatika\"])\n",
    "        out.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071988f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = f\"USER: {s['user']}\\nCOMPATIKA:\"\n",
    "target_text = s['compatika']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c667f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aman\\AppData\\Local\\Temp\\ipykernel_24468\\4247225922.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"compatika_v1alpha_scratch.pt\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompatika_v1alpha_scratch.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m      4\u001b[0m train_more_epochs(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "state_dict = torch.load(\"compatika_v1alpha_scratch.pt\")\n",
    "model.load_state_dict(state_dict)\n",
    "train_more_epochs(...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92194dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned dataset saved with 76338 unique pairs.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "unique = set()\n",
    "cleaned = []\n",
    "with open(\"data/train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        d = json.loads(line)\n",
    "        pair = (d[\"user\"].strip().lower(), d[\"compatika\"].strip().lower())\n",
    "        if pair not in unique:\n",
    "            unique.add(pair)\n",
    "            cleaned.append(d)\n",
    "\n",
    "with open(\"data/train_clean.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for d in cleaned:\n",
    "        f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Cleaned dataset saved with {len(cleaned)} unique pairs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b6b5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.replace(\"_comma\", \",\")\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "# Apply cleaning to both fields before saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a2ca886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "lines = open(\"data/train_clean.jsonl\", \"r\", encoding=\"utf-8\").readlines()\n",
    "random.shuffle(lines)\n",
    "open(\"data/train_shuffled.jsonl\", \"w\", encoding=\"utf-8\").writelines(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73012324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/train_clean.jsonl\", \"r\", encoding=\"utf-8\") as f, \\\n",
    "     open(\"data/text_for_tok.txt\", \"w\", encoding=\"utf-8\") as out:\n",
    "    for line in f:\n",
    "        sample = json.loads(line)\n",
    "        out.write(sample[\"user\"] + \"\\n\")\n",
    "        out.write(sample[\"compatika\"] + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b991d25",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Permission denied: \"tok/compatika_sp.model\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentencepiece\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspm\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mspm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/text_for_tok.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtok/compatika_sp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munigram\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcharacter_coverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Tokenizer retrained and saved as tok/compatika_sp.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aman\\Desktop\\Compatika-V1-alpha-ai-model-in-2-4-MB-dataset-and-1m-param\\env\\lib\\site-packages\\sentencepiece\\__init__.py:1047\u001b[0m, in \u001b[0;36mSentencePieceTrainer.Train\u001b[1;34m(arg, logstream, **kwargs)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mTrain\u001b[39m(arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, logstream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1046\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m _LogStream(ostream\u001b[38;5;241m=\u001b[39mlogstream):\n\u001b[1;32m-> 1047\u001b[0m     SentencePieceTrainer\u001b[38;5;241m.\u001b[39m_Train(arg\u001b[38;5;241m=\u001b[39marg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aman\\Desktop\\Compatika-V1-alpha-ai-model-in-2-4-MB-dataset-and-1m-param\\env\\lib\\site-packages\\sentencepiece\\__init__.py:1040\u001b[0m, in \u001b[0;36mSentencePieceTrainer._Train\u001b[1;34m(arg, **kwargs)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SentencePieceTrainer\u001b[38;5;241m.\u001b[39m_TrainFromMap2(new_kwargs, sentence_iterator)\n\u001b[0;32m   1039\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_TrainFromMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aman\\Desktop\\Compatika-V1-alpha-ai-model-in-2-4-MB-dataset-and-1m-param\\env\\lib\\site-packages\\sentencepiece\\__init__.py:985\u001b[0m, in \u001b[0;36mSentencePieceTrainer._TrainFromMap\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_TrainFromMap\u001b[39m(args):\n\u001b[1;32m--> 985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceTrainer__TrainFromMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Permission denied: \"tok/compatika_sp.model\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input='data/text_for_tok.txt',\n",
    "    model_prefix='tok/compatika_sp',\n",
    "    vocab_size=10000,\n",
    "    model_type='unigram',\n",
    "    character_coverage=1.0,\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3\n",
    ")\n",
    "print(\"‚úÖ Tokenizer retrained and saved as tok/compatika_sp.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d9a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [4, 72, 771, 45, 254, 5]\n",
      "Decoded: I feel anxious about tomorrow.\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor(model_file=\"tok/compatika_sp.model\")\n",
    "text = \"I feel anxious about tomorrow.\"\n",
    "print(\"Encoded:\", sp.encode(text, out_type=int))\n",
    "print(\"Decoded:\", sp.decode(sp.encode(text, out_type=int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5c7ada5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer trained successfully!\n",
      "üìÑ Files generated: tok/compatika_sp.model and tok/compatika_sp.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "# Create output folder if not exists\n",
    "os.makedirs(\"tok\", exist_ok=True)\n",
    "\n",
    "# Train tokenizer\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input='data/text_for_tok.txt',          # your dataset text\n",
    "    model_prefix='tok/compatika_sp',        # output files prefix\n",
    "    vocab_size=10000,                       # vocabulary size\n",
    "    model_type='unigram',                   # good for dialogue data\n",
    "    character_coverage=1.0,                 # cover all characters\n",
    "    pad_id=0, unk_id=1, bos_id=2, eos_id=3  # reserved IDs\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tokenizer trained successfully!\")\n",
    "print(\"üìÑ Files generated: tok/compatika_sp.model and tok/compatika_sp.vocab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71e10da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vocab size: 10000\n",
      "Encoded IDs: [4, 72, 771, 45, 254, 5]\n",
      "Decoded text: I feel anxious about tomorrow.\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=\"tok/compatika_sp.model\")\n",
    "print(\"‚úÖ Vocab size:\", sp.get_piece_size())\n",
    "\n",
    "sample_text = \"I feel anxious about tomorrow.\"\n",
    "encoded = sp.encode(sample_text, out_type=int)\n",
    "decoded = sp.decode(encoded)\n",
    "\n",
    "print(\"Encoded IDs:\", encoded)\n",
    "print(\"Decoded text:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baecad32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Split complete!\n",
      "Train samples: 68940\n",
      "Val samples:   7660\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "input_path = \"data/train_clean.jsonl\"   # or your main dataset file\n",
    "train_out = \"data/train.jsonl\"\n",
    "val_out = \"data/val.jsonl\"\n",
    "\n",
    "# 1Ô∏è‚É£ Read all samples\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    samples = [json.loads(line) for line in f]\n",
    "\n",
    "# 2Ô∏è‚É£ Shuffle for randomness\n",
    "random.shuffle(samples)\n",
    "\n",
    "# 3Ô∏è‚É£ Split 90% train / 10% val (you can change ratio)\n",
    "split_idx = int(0.9 * len(samples))\n",
    "train_samples = samples[:split_idx]\n",
    "val_samples = samples[split_idx:]\n",
    "\n",
    "# 4Ô∏è‚É£ Write to new files\n",
    "with open(train_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in train_samples:\n",
    "        f.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(val_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in val_samples:\n",
    "        f.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Split complete!\")\n",
    "print(f\"Train samples: {len(train_samples)}\")\n",
    "print(f\"Val samples:   {len(val_samples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db776e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
