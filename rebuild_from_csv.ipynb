{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f90479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train.csv: 76673it [00:00, 84456.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 76600 examples to data/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing valid.csv: 12030it [00:00, 86447.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 12019 examples to data/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test.csv: 10943it [00:00, 81132.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10932 examples to data/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# scripts/rebuild_from_csv.py\n",
    "import csv, json, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "def csv_to_jsonl(csv_path, out_path, user_col=\"context\", reply_col=\"utterance\", max_reply_words=80):\n",
    "    written = 0\n",
    "    with open(csv_path, newline='', encoding='utf-8') as f_in, open(out_path, 'w', encoding='utf-8') as f_out:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        for row in tqdm(reader, desc=f\"Processing {os.path.basename(csv_path)}\"):\n",
    "            user = row.get(user_col, \"\").strip()\n",
    "            reply = row.get(reply_col, \"\").strip()\n",
    "            if not user or not reply:\n",
    "                continue\n",
    "            # basic normalization of whitespace\n",
    "            user = \" \".join(user.split())\n",
    "            reply = \" \".join(reply.split())\n",
    "            # filter extremely long replies (optional)\n",
    "            if len(reply.split()) > max_reply_words:\n",
    "                continue\n",
    "            j = {\"user\": user, \"compatika\": reply}\n",
    "            f_out.write(json.dumps(j, ensure_ascii=False) + \"\\n\")\n",
    "            written += 1\n",
    "    print(f\"Saved {written} examples to {out_path}\")\n",
    "\n",
    "# adjust paths to your CSV files\n",
    "csv_to_jsonl(\"datas/empathetic_dialogues/train.csv\", \"data/train.jsonl\")\n",
    "csv_to_jsonl(\"datas/empathetic_dialogues/valid.csv\", \"data/val.jsonl\")\n",
    "csv_to_jsonl(\"datas/empathetic_dialogues/test.csv\", \"data/test.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa93b0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76600it [00:02, 37178.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 76600 cleaned examples to data/cleaned/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12019it [00:00, 37222.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 12019 cleaned examples to data/cleaned/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10932it [00:00, 35804.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10932 cleaned examples to data/cleaned/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# scripts/clean_jsonl.py\n",
    "import json, os, re\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs(\"data/cleaned\", exist_ok=True)\n",
    "\n",
    "REPLACEMENTS = [\n",
    "    (r\"_comma_\", \",\"),\n",
    "    (r\"_apostrophe_\", \"'\"),\n",
    "    (r\"_quote_\", '\"'),\n",
    "    (r\" +,\", \",\"),        # spaces before commas\n",
    "    (r\"\\s+([?.!,])\", r\"\\1\"), # fix spacing before punctuation\n",
    "]\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.strip()\n",
    "    for pat, repl in REPLACEMENTS:\n",
    "        s = re.sub(pat, repl, s)\n",
    "    # fix multiple spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    # fix spacing around parentheses and stray underscores\n",
    "    s = s.replace(\" _comma_\", \",\").replace(\"_\", \"\")\n",
    "    return s.strip()\n",
    "\n",
    "def clean_file(infile, outfile, max_reply_words=80):\n",
    "    kept = 0\n",
    "    with open(infile, 'r', encoding='utf-8') as fi, open(outfile, 'w', encoding='utf-8') as fo:\n",
    "        for line in tqdm(fi):\n",
    "            try:\n",
    "                j = json.loads(line)\n",
    "                user = clean_text(j.get(\"user\",\"\"))\n",
    "                reply = clean_text(j.get(\"compatika\",\"\"))\n",
    "                if not user or not reply:\n",
    "                    continue\n",
    "                # enforce reply length limit\n",
    "                if len(reply.split()) > max_reply_words:\n",
    "                    # shorten by taking first 40 words + \"...\"\n",
    "                    reply = \" \".join(reply.split()[:40]).strip() + \"...\"\n",
    "                fo.write(json.dumps({\"user\": user, \"compatika\": reply}, ensure_ascii=False) + \"\\n\")\n",
    "                kept += 1\n",
    "            except Exception:\n",
    "                continue\n",
    "    print(f\"Saved {kept} cleaned examples to {outfile}\")\n",
    "\n",
    "# clean all rebuilt files\n",
    "clean_file(\"data/train.jsonl\", \"data/cleaned/train.jsonl\")\n",
    "clean_file(\"data/val.jsonl\", \"data/cleaned/val.jsonl\")\n",
    "clean_file(\"data/test.jsonl\", \"data/cleaned/test.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d50f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76600it [00:00, 248655.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deduped count: 76066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# scripts/dedupe.py\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "seen = set()\n",
    "outf = open(\"data/cleaned/train_dedup.jsonl\",\"w\",encoding=\"utf-8\")\n",
    "count=0\n",
    "with open(\"data/cleaned/train.jsonl\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        j = json.loads(line)\n",
    "        key = (j[\"user\"], j[\"compatika\"])\n",
    "        if key in seen: continue\n",
    "        seen.add(key)\n",
    "        outf.write(line)\n",
    "        count+=1\n",
    "outf.close()\n",
    "print(\"deduped count:\", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7546cad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- preview data/cleaned/train.jsonl ----\n",
      "Sample 1:\n",
      " USER: sentimental\n",
      " COMPATIKA: I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\n",
      "\n",
      "Sample 2:\n",
      " USER: sentimental\n",
      " COMPATIKA: Was this a friend you were in love with, or just a best friend?\n",
      "\n",
      "Sample 3:\n",
      " USER: sentimental\n",
      " COMPATIKA: This was a best friend. I miss her.\n",
      "\n",
      "Sample 4:\n",
      " USER: sentimental\n",
      " COMPATIKA: Where has she gone?\n",
      "\n",
      "Sample 5:\n",
      " USER: sentimental\n",
      " COMPATIKA: We no longer talk.\n",
      "\n",
      "---- preview data/cleaned/val.jsonl ----\n",
      "Sample 1:\n",
      " USER: terrified\n",
      " COMPATIKA: Today,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\n",
      "\n",
      "Sample 2:\n",
      " USER: terrified\n",
      " COMPATIKA: Are you fine now?\n",
      "\n",
      "Sample 3:\n",
      " USER: terrified\n",
      " COMPATIKA: Yeah,i'm doing alright now, but with minor injuries.\n",
      "\n",
      "Sample 4:\n",
      " USER: terrified\n",
      " COMPATIKA: Cool :) Is your car damaged a lot?\n",
      "\n",
      "Sample 5:\n",
      " USER: terrified\n",
      " COMPATIKA: The car was badly damaged,i veered outside the road and hit a tree trunk. next thing is insurance follow up.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for i, path in enumerate([\"data/cleaned/train.jsonl\",\"data/cleaned/val.jsonl\"]):\n",
    "    print(\"---- preview\", path, \"----\")\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        for n,line in enumerate(f):\n",
    "            if n>=5: break\n",
    "            j=json.loads(line)\n",
    "            print(f\"Sample {n+1}:\\n USER: {j['user']}\\n COMPATIKA: {j['compatika']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c911ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags']\n",
      "First row example: ['hit:0_conv:1', '1', 'sentimental', 'I remember going to the fireworks with my best friend. There was a lot of people_comma_ but it only felt like us in the world.', '1', 'I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people_comma_ we felt like the only people in the world.', '5|5|5_2|2|5', '']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"datas/empathetic_dialogues/train.csv\", newline='', encoding='utf-8') as f:\n",
    "    r = csv.reader(f)\n",
    "    headers = next(r)\n",
    "    print(\"Headers:\", headers)\n",
    "    first = next(r)\n",
    "    print(\"First row example:\", first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dad6ca3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train.csv: 76673it [00:00, 89291.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 76600 examples to data/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing valid.csv: 12030it [00:00, 59126.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 12019 examples to data/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test.csv: 10943it [00:00, 81206.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 10932 examples to data/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv, json, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "def csv_to_jsonl(csv_path, out_path, user_col=\"prompt\", reply_col=\"utterance\", max_reply_words=80):\n",
    "    written = 0\n",
    "    with open(csv_path, newline='', encoding='utf-8') as f_in, open(out_path, 'w', encoding='utf-8') as f_out:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        for row in tqdm(reader, desc=f\"Processing {os.path.basename(csv_path)}\"):\n",
    "            user = row.get(user_col, \"\").strip()\n",
    "            reply = row.get(reply_col, \"\").strip()\n",
    "            if not user or not reply:\n",
    "                continue\n",
    "            # remove _comma_ etc\n",
    "            user = user.replace(\"_comma_\", \",\")\n",
    "            reply = reply.replace(\"_comma_\", \",\")\n",
    "            if len(reply.split()) > max_reply_words:\n",
    "                continue\n",
    "            j = {\"user\": user, \"compatika\": reply}\n",
    "            f_out.write(json.dumps(j, ensure_ascii=False) + \"\\n\")\n",
    "            written += 1\n",
    "    print(f\"âœ… Saved {written} examples to {out_path}\")\n",
    "\n",
    "# update paths as needed\n",
    "csv_to_jsonl(\"datas/empathetic_dialogues/train.csv\", \"data/train.jsonl\")\n",
    "csv_to_jsonl(\"datas/empathetic_dialogues/valid.csv\", \"data/val.jsonl\")\n",
    "csv_to_jsonl(\"datas/empathetic_dialogues/test.csv\", \"data/test.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d17e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      " USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      " COMPATIKA: I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\n",
      "\n",
      "Sample 2:\n",
      " USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      " COMPATIKA: Was this a friend you were in love with, or just a best friend?\n",
      "\n",
      "Sample 3:\n",
      " USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      " COMPATIKA: This was a best friend. I miss her.\n",
      "\n",
      "Sample 4:\n",
      " USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      " COMPATIKA: Where has she gone?\n",
      "\n",
      "Sample 5:\n",
      " USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      " COMPATIKA: We no longer talk.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        j = json.loads(line)\n",
    "        print(f\"Sample {i+1}:\\n USER: {j['user']}\\n COMPATIKA: {j['compatika']}\\n\")\n",
    "        if i >= 4:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67583d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“– Reading data/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train.jsonl: 76600it [00:01, 48746.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“– Reading data/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val.jsonl: 12019it [00:00, 46605.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Removing duplicates...\n",
      "ðŸ§¾ Writing 88357 unique lines to data/text_for_tok.txt\n",
      "âœ… Done! Your tokenizer training file is ready at: data/text_for_tok.txt\n"
     ]
    }
   ],
   "source": [
    "# scripts/make_text_for_tokenizer.py\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Input and output paths\n",
    "input_files = [\n",
    "    \"data/train.jsonl\",\n",
    "    \"data/val.jsonl\"\n",
    "]\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "output_path = \"data/text_for_tok.txt\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic cleaning and normalization for tokenizer data\"\"\"\n",
    "    text = text.strip()\n",
    "    text = text.replace(\"_comma_\", \",\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s([?.!,])\", r\"\\1\", text)\n",
    "    text = text.replace(\" ,\", \",\")\n",
    "    text = text.replace(\" .\", \".\")\n",
    "    return text.strip()\n",
    "\n",
    "lines = []\n",
    "for path in input_files:\n",
    "    print(f\"ðŸ“– Reading {path}\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=f\"Processing {os.path.basename(path)}\"):\n",
    "            try:\n",
    "                j = json.loads(line)\n",
    "                user = clean_text(j.get(\"user\", \"\"))\n",
    "                compatika = clean_text(j.get(\"compatika\", \"\"))\n",
    "                if not user or not compatika:\n",
    "                    continue\n",
    "                # Merge user + compatika into one line\n",
    "                merged = f\"{user} {compatika}\"\n",
    "                lines.append(merged)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "# Remove duplicates\n",
    "print(f\"ðŸ§¹ Removing duplicates...\")\n",
    "unique_lines = list(set(lines))\n",
    "print(f\"ðŸ§¾ Writing {len(unique_lines)} unique lines to {output_path}\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "    for line in unique_lines:\n",
    "        out.write(line.strip() + \"\\n\")\n",
    "\n",
    "print(\"âœ… Done! Your tokenizer training file is ready at:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08540f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I broked something at home and everyonw blamed my brother. I didn't say anything. I feel bad about it. That is terrible! Why did you blame him?\n",
      "i like person who are honestly with me That's a good way of saying live a good life.\n",
      "I stole from my parents as a kid and got caught. I felt so bad afterwards. I would feel guilty too if I was in your shoes. Did you feel bad because you stole or bad because you got caught?\n",
      "Doing a race with the wrong shoes. Good thing you found someone who had as big of feet as you do!\n",
      "I am so scary that my manager doesn't keep his promise for my promotion. No, no reason to he will do it.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/text_for_tok.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33ec1ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“– Reading data/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train.jsonl: 76600it [00:01, 51290.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“– Reading data/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val.jsonl: 12019it [00:00, 52296.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¾ Writing 102044 unique lines to data/text_for_tok.txt\n",
      "âœ… Clean tokenizer file ready: data/text_for_tok.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# scripts/make_text_for_tokenizer.py (clean single-line version)\n",
    "import json, os, re\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "input_files = [\"data/train.jsonl\", \"data/val.jsonl\"]\n",
    "output_path = \"data/text_for_tok.txt\"\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"_comma_\", \",\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s([?.!,])\", r\"\\1\", text)\n",
    "    text = text.replace(\" ,\", \",\").replace(\" .\", \".\")\n",
    "    return text.strip()\n",
    "\n",
    "lines = set()\n",
    "for path in input_files:\n",
    "    print(f\"ðŸ“– Reading {path}\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=f\"Processing {os.path.basename(path)}\"):\n",
    "            try:\n",
    "                j = json.loads(line)\n",
    "                user = clean_text(j.get(\"user\", \"\"))\n",
    "                compatika = clean_text(j.get(\"compatika\", \"\"))\n",
    "                if not user or not compatika:\n",
    "                    continue\n",
    "                # add each sentence separately (not merged)\n",
    "                lines.add(user)\n",
    "                lines.add(compatika)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "print(f\"ðŸ§¾ Writing {len(lines)} unique lines to {output_path}\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"âœ… Clean tokenizer file ready:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad67b6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want them to be able to go to the store and buy everything with no worries.\n",
      "I was worried driving home tonight. People were swerving all over\n",
      "I had an interview for a hospital I want to volunteer at but I felt a little under-qualified\n",
      "Sorry about caps lock i didn't realize. And yea i think i'm going to propose here shortly.\n",
      "I was sad when my cat died. He was so sick\n",
      "The barber shop is totally worth it. They do such a good job.\n",
      "Wow what kind of toys did you find\n",
      "i was pissed when i saw someone left my gate open\n",
      "Got seasons pass to an amusement park.\n",
      "Yeah, that sounds like a really good idea!\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/text_for_tok.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ef349aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking data/train.jsonl: 76600it [00:00, 260384.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Checked 76600 lines. Issues found: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking data/val.jsonl: 12019it [00:00, 207128.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Checked 12019 lines. Issues found: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def verify_jsonl(path):\n",
    "    issues = 0\n",
    "    total = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(tqdm(f, desc=f\"Checking {path}\")):\n",
    "            total += 1\n",
    "            try:\n",
    "                j = json.loads(line)\n",
    "                if not isinstance(j.get(\"user\"), str) or not isinstance(j.get(\"compatika\"), str):\n",
    "                    print(f\"âŒ Line {i+1}: non-string fields\")\n",
    "                    issues += 1\n",
    "                if not j[\"user\"].strip() or not j[\"compatika\"].strip():\n",
    "                    print(f\"âš ï¸ Line {i+1}: empty text\")\n",
    "                    issues += 1\n",
    "                if \"_comma_\" in j[\"user\"] or \"_comma_\" in j[\"compatika\"]:\n",
    "                    print(f\"âš ï¸ Line {i+1}: found _comma_ artifact\")\n",
    "                    issues += 1\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Line {i+1}: invalid JSON ({e})\")\n",
    "                issues += 1\n",
    "    print(f\"\\nâœ… Checked {total} lines. Issues found: {issues}\")\n",
    "\n",
    "verify_jsonl(\"data/train.jsonl\")\n",
    "verify_jsonl(\"data/val.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edfce3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      "COMPATIKA: I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\n",
      "\n",
      "USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      "COMPATIKA: Was this a friend you were in love with, or just a best friend?\n",
      "\n",
      "USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      "COMPATIKA: This was a best friend. I miss her.\n",
      "\n",
      "USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      "COMPATIKA: Where has she gone?\n",
      "\n",
      "USER: I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.\n",
      "COMPATIKA: We no longer talk.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"data/train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        j = json.loads(line)\n",
    "        print(f\"USER: {j['user']}\")\n",
    "        print(f\"COMPATIKA: {j['compatika']}\\n\")\n",
    "        if i >= 4: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec2e07ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vocab size: 10000\n",
      "Encoded IDs: [5, 87, 9, 364, 112, 204, 4]\n",
      "Decoded text: I feel a bit anxious today.\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"tok/compatika_sp.model\")\n",
    "\n",
    "print(\"âœ… Vocab size:\", sp.GetPieceSize())\n",
    "sample = \"I feel a bit anxious today.\"\n",
    "ids = sp.EncodeAsIds(sample)\n",
    "print(\"Encoded IDs:\", ids)\n",
    "print(\"Decoded text:\", sp.DecodeIds(ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5349e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want them to be able to go to the store and buy everything with no worries.\n",
      "I was worried driving home tonight. People were swerving all over\n",
      "I had an interview for a hospital I want to volunteer at but I felt a little under-qualified\n",
      "Sorry about caps lock i didn't realize. And yea i think i'm going to propose here shortly.\n",
      "I was sad when my cat died. He was so sick\n",
      "The barber shop is totally worth it. They do such a good job.\n",
      "Wow what kind of toys did you find\n",
      "i was pissed when i saw someone left my gate open\n",
      "Got seasons pass to an amusement park.\n",
      "Yeah, that sounds like a really good idea!\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/text_for_tok.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22d6264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
