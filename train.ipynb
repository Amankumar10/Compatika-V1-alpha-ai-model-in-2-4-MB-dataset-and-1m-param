{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e9449d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPATIKA V1-ALPHA TRAINING + TESTING (PURE PYTORCH)\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fe4b8b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer loaded. Vocab size: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 1Ô∏è‚É£ Load tokenizer\n",
    "# ============================================================\n",
    "sp = spm.SentencePieceProcessor(model_file=\"tok/compatika_sp.model\")\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(\"‚úÖ Tokenizer loaded. Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78284005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£ Model definition\n",
    "# ============================================================\n",
    "class CompatikaModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=192, n_heads=4, n_layers=3, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            activation=\"gelu\"\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.permute(1, 0, 2)   # (seq, batch, embed)\n",
    "        x = self.transformer(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x.permute(1, 0, 2)  # (batch, seq, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "124414cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 68940 train samples and 7660 val samples.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3Ô∏è‚É£ Dataset loading\n",
    "# ============================================================\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.samples = [json.loads(line) for line in f]\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        # Input: full conversation up to COMPATIKA\n",
    "        inp_text  = f\"USER: {s['user']}\\nCOMPATIKA:\"\n",
    "        # Target: only the reply\n",
    "        out_text  = s['compatika']\n",
    "\n",
    "        x = torch.tensor(sp.encode(inp_text, out_type=int), dtype=torch.long)\n",
    "        y = torch.tensor(sp.encode(out_text, out_type=int), dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    x = nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=0)\n",
    "    y = nn.utils.rnn.pad_sequence(ys, batch_first=True, padding_value=0)\n",
    "    min_len = min(x.size(1), y.size(1))\n",
    "    x = x[:, :min_len]\n",
    "    y = y[:, :min_len]\n",
    "    return x, y\n",
    "train_data = ChatDataset(\"data/train.jsonl\")\n",
    "val_data = ChatDataset(\"data/val.jsonl\")\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_data, batch_size=8, collate_fn=collate)\n",
    "\n",
    "print(f\"Loaded {len(train_data)} train samples and {len(val_data)} val samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fa124b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4Ô∏è‚É£ Training setup\n",
    "# ============================================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CompatikaModel(vocab_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "08af19d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8618/8618 [01:33<00:00, 92.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 5.9870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8618/8618 [01:33<00:00, 91.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 5.8091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8618/8618 [01:33<00:00, 91.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 5.7066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8618/8618 [01:41<00:00, 85.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 5.6269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8618/8618 [01:44<00:00, 82.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 5.5538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8618/8618 [01:33<00:00, 91.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 5.4896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8618/8618 [01:34<00:00, 91.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 5.4331\n",
      "Val loss: 5.7200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5Ô∏è‚É£ Training loop\n",
    "# ============================================================\n",
    "epochs = 7\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out.reshape(-1, vocab_size), y.reshape(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Train loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out.reshape(-1, vocab_size), y.reshape(-1))\n",
    "        val_loss += loss.item()\n",
    "print(f\"Val loss: {val_loss / len(val_loader):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f2ba5aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved as compatika_v1alpha_scratch.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 6Ô∏è‚É£ Save model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "torch.save(model.state_dict(), \"compatika_v1alpha_scratch.pt\")\n",
    "print(\"‚úÖ Model saved as compatika_v1alpha_scratch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ac8af268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model and tokenizer loaded successfully for testing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aman\\AppData\\Local\\Temp\\ipykernel_11100\\3364364285.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"compatika_v1alpha_scratch.pt\", map_location=\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7Ô∏è‚É£ Reload model for testing\n",
    "# ============================================================\n",
    "model = CompatikaModel(vocab_size)\n",
    "model.load_state_dict(torch.load(\"compatika_v1alpha_scratch.pt\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "print(\"\\n‚úÖ Model and tokenizer loaded successfully for testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1371439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_len=60, temperature=0.8, top_p=0.9, repetition_penalty=1.3):\n",
    "    # ensure clean input\n",
    "    prompt = prompt.strip().replace(\"\\n\", \" \")\n",
    "    tokens = sp.encode(prompt, out_type=int)\n",
    "    x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)\n",
    "    generated = set(tokens)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out = model(x)\n",
    "\n",
    "        logits = out[0, -1] / temperature\n",
    "        logits = torch.clamp(logits, -20, 20)\n",
    "\n",
    "        for t in generated:\n",
    "            logits[t] /= repetition_penalty\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_probs[sorted_indices_to_remove] = 0\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "\n",
    "        next_id = torch.multinomial(sorted_probs, 1).item()\n",
    "        tokens.append(next_id)\n",
    "        generated.add(next_id)\n",
    "        if next_id == 3:\n",
    "            break\n",
    "\n",
    "        x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    text = sp.decode(tokens)\n",
    "\n",
    "    # remove USER/COMPATIKA echoes and stray symbols\n",
    "    text = text.replace(\"USER:\", \"\").replace(\"COMPATIKA:\", \"\").replace(\"_comma\", \",\")\n",
    "    text = text.replace(\"‚Åá\", \"\").strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ff6ff80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: I'm really upset. My friend ignored my message.\n",
      "Compatika: I'm really upset. My friend ignored my message.  theyed they It   being get much my.' my\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üß™ TEST MODE (for Jupyter)\n",
    "# ============================================================\n",
    "\n",
    "test_inputs = [\n",
    "   \n",
    "  \"I'm really upset. My friend ignored my message.\"\n",
    " \n",
    "]\n",
    "\n",
    "\n",
    "for user_input in test_inputs:\n",
    "    prompt = f\"USER: {user_input}\\nCOMPATIKA:\"\n",
    "    reply = generate_response(prompt)\n",
    "    print(f\"USER: {user_input}\")\n",
    "    print(f\"Compatika: {reply}\\n{'-'*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a93b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m      8\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSER: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCOMPATIKA:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompatika:\u001b[39m\u001b[38;5;124m\"\u001b[39m, reply)\n",
      "Cell \u001b[1;32mIn[39], line 25\u001b[0m, in \u001b[0;36mgenerate_response\u001b[1;34m(prompt, max_len, temperature, top_p, repetition_penalty)\u001b[0m\n\u001b[0;32m     22\u001b[0m sorted_probs[sorted_indices_to_remove] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     23\u001b[0m sorted_probs \u001b[38;5;241m=\u001b[39m sorted_probs \u001b[38;5;241m/\u001b[39m sorted_probs\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m---> 25\u001b[0m next_id \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43msorted_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     26\u001b[0m tokens\u001b[38;5;241m.\u001b[39mappend(next_id)\n\u001b[0;32m     27\u001b[0m generated\u001b[38;5;241m.\u001b[39madd(next_id)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f8a7166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 87, 112, 204, 4]\n",
      "I feel anxious today.\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode(\"I feel anxious today.\", out_type=int))\n",
    "print(sp.decode(sp.encode(\"I feel anxious today.\", out_type=int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67ef95dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick example cleaning step\n",
    "import re, json\n",
    "def clean_text(t):\n",
    "    t = t.replace(\"_comma\", \",\")\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = re.sub(r\"[0-9]+\\|[0-9]+\\|[0-9]+[_0-9|]*\", \"\", t)  # remove rating patterns\n",
    "    t = re.sub(r\":\\d+\", \"\", t)  # remove stray numeric tokens\n",
    "    return t.strip()\n",
    "\n",
    "with open(\"data/train.jsonl\",\"r\",encoding=\"utf-8\") as f, open(\"data/train_clean.jsonl\",\"w\",encoding=\"utf-8\") as out:\n",
    "    for line in f:\n",
    "        s = json.loads(line)\n",
    "        s[\"user\"] = clean_text(s[\"user\"])\n",
    "        s[\"compatika\"] = clean_text(s[\"compatika\"])\n",
    "        out.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92bcbad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = f\"USER: {s['user']}\\nCOMPATIKA:\"\n",
    "target_text = s['compatika']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f729c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aman\\AppData\\Local\\Temp\\ipykernel_11100\\63374353.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"compatika_v1alpha_scratch.pt\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'compatika_v1alpha_scratch.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompatika_v1alpha_scratch.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n",
      "File \u001b[1;32mc:\\Users\\aman\\Desktop\\Compatika-V1-alpha-ai-model-in-2-4-MB-dataset-and-1m-param\\env\\lib\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\aman\\Desktop\\Compatika-V1-alpha-ai-model-in-2-4-MB-dataset-and-1m-param\\env\\lib\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\aman\\Desktop\\Compatika-V1-alpha-ai-model-in-2-4-MB-dataset-and-1m-param\\env\\lib\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'compatika_v1alpha_scratch.pt'"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(\"compatika_v1alpha_scratch.pt\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b76e99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I feel anxious today.\n"
     ]
    }
   ],
   "source": [
    "txt = \"I feel anxious today.\"\n",
    "print(sp.decode(sp.encode(txt, out_type=int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f0a42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
