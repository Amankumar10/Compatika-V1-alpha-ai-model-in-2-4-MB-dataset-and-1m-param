{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8246466e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aman\\Desktop\\Compatika-V1-alpha-ai-model-in-2-4-MB-dataset-and-1m-param\\env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\aman\\AppData\\Local\\Temp\\ipykernel_7736\\1014568064.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"compatika_v1alpha_scratch.pt\", map_location=\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Prompt: USER: I'm really upset. My friend ignored my message.\n",
      "COMPATIKA:\n",
      "Compatika: USER: I'm really upset. My friend ignored my message. COMPATIKA: My friend'm really upset. I'm really upset. I'm really upset. I'm really upset. I'm really upset. I'm really upset. I'm\n",
      "\n",
      "============================================================\n",
      "Prompt: USER: I feel anxious about tomorrow.\n",
      "COMPATIKA:\n",
      "Compatika: USER: I feel anxious about tomorrow. COMPATIKA: I feel anxious about tomorrow about tomorrow about tomorrow about tomorrow about tomorrow about tomorrow about tomorrow about tomorrow about tomorrow about tomorrow about tomorrow about tomorrow about tomorrow about tomorrow about tomorrow about tomorrow about tomorrow about tomorrow about\n",
      "\n",
      "============================================================\n",
      "Prompt: USER: I want to tell my partner I need more time together.\n",
      "COMPATIKA:\n",
      "Compatika: USER: I want to tell my partner I need more time together. COMPATIKA: I need to tell my partner. I need to tell my partner. I need to tell my partner. I need to tell my partner. I need to tell my partner. I need to tell my\n",
      "\n",
      "============================================================\n",
      "Prompt: USER: I feel guilty because I blamed someone unfairly.\n",
      "COMPATIKA:\n",
      "Compatika: USER: I feel guilty because I blamed someone unfairly. COMPATIKA: I feel guilty because I feel guilty because I feel guilty because I feel guilty because I feel guilty because I feel guilty because I feel guilty because I feel guilty because I feel guilty because I feel guilty because\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sentencepiece as spm\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------------- MODEL DEFINITION ----------------\n",
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim * 4\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        emb = emb.permute(1, 0, 2)\n",
    "        encoded = self.encoder(emb)\n",
    "        out = self.fc(encoded)\n",
    "        return out.permute(1, 0, 2)\n",
    "\n",
    "# ---------------- LOAD TOKENIZER ----------------\n",
    "sp = spm.SentencePieceProcessor(model_file=\"tok/compatika_sp.model\")\n",
    "vocab_size = sp.get_piece_size()\n",
    "\n",
    "# ---------------- LOAD MODEL ----------------\n",
    "model = TinyTransformer(vocab_size)\n",
    "model.load_state_dict(torch.load(\"compatika_v1alpha_scratch.pt\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "# ---------------- GENERATION FUNCTION ----------------\n",
    "def generate_response(prompt, max_len=40):\n",
    "    tokens = sp.encode(prompt, out_type=int)\n",
    "    x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out = model(x)\n",
    "        next_id = out[0, -1].argmax().item()\n",
    "        tokens.append(next_id)\n",
    "        if next_id == 3:  # EOS token\n",
    "            break\n",
    "        x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)\n",
    "    return sp.decode(tokens)\n",
    "\n",
    "# ---------------- TEST ON FEW PROMPTS ----------------\n",
    "tests = [\n",
    "    \"USER: I'm really upset. My friend ignored my message.\\nCOMPATIKA:\",\n",
    "    \"USER: I feel anxious about tomorrow.\\nCOMPATIKA:\",\n",
    "    \"USER: I want to tell my partner I need more time together.\\nCOMPATIKA:\",\n",
    "    \"USER: I feel guilty because I blamed someone unfairly.\\nCOMPATIKA:\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Prompt:\", t)\n",
    "    print(\"Compatika:\", generate_response(t))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d52e3a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model and tokenizer loaded successfully\n",
      "Vocab size: 10000\n",
      "======================================================================\n",
      "Prompt: USER: I'm really upset. My friend ignored my message.\n",
      "COMPATIKA:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aman\\AppData\\Local\\Temp\\ipykernel_7736\\67979827.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"compatika_v1alpha_scratch.pt\", map_location=\"cpu\"))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 83\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt:\u001b[39m\u001b[38;5;124m\"\u001b[39m, t)\n\u001b[1;32m---> 83\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompatika:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[1;32mIn[9], line 61\u001b[0m, in \u001b[0;36mgenerate_response\u001b[1;34m(prompt, max_len, temperature, top_p)\u001b[0m\n\u001b[0;32m     58\u001b[0m sorted_probs[sorted_indices_to_remove] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     59\u001b[0m sorted_probs \u001b[38;5;241m=\u001b[39m sorted_probs \u001b[38;5;241m/\u001b[39m sorted_probs\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m---> 61\u001b[0m next_id \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43msorted_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     62\u001b[0m tokens\u001b[38;5;241m.\u001b[39mappend(next_id)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:  \u001b[38;5;66;03m# EOS token\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sentencepiece as spm\n",
    "\n",
    "# ============================================================\n",
    "# 1ï¸âƒ£ Model Definition (same as during training)\n",
    "# ============================================================\n",
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        emb = emb.permute(1, 0, 2)\n",
    "        encoded = self.encoder(emb)\n",
    "        out = self.fc(encoded)\n",
    "        return out.permute(1, 0, 2)\n",
    "\n",
    "# ============================================================\n",
    "# 2ï¸âƒ£ Load Tokenizer and Model Weights\n",
    "# ============================================================\n",
    "sp = spm.SentencePieceProcessor(model_file=\"tok/compatika_sp.model\")\n",
    "vocab_size = sp.get_piece_size()\n",
    "\n",
    "model = TinyTransformer(vocab_size)\n",
    "model.load_state_dict(torch.load(\"compatika_v1alpha_scratch.pt\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Model and tokenizer loaded successfully\")\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "# ============================================================\n",
    "# 3ï¸âƒ£ Generation Function (with temperature + top-p sampling)\n",
    "# ============================================================\n",
    "def generate_response(prompt, max_len=50, temperature=0.8, top_p=0.9):\n",
    "    tokens = sp.encode(prompt, out_type=int)\n",
    "    x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out = model(x)\n",
    "        logits = out[0, -1] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # nucleus (top-p) sampling\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_probs[sorted_indices_to_remove] = 0\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "\n",
    "        next_id = torch.multinomial(sorted_probs, 1).item()\n",
    "        tokens.append(next_id)\n",
    "\n",
    "        if next_id == 3:  # EOS token\n",
    "            break\n",
    "        x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    return sp.decode(tokens)\n",
    "\n",
    "# ============================================================\n",
    "# 4ï¸âƒ£ Test on Multiple Prompts\n",
    "# ============================================================\n",
    "tests = [\n",
    "    \"USER: I'm really upset. My friend ignored my message.\\nCOMPATIKA:\",\n",
    "    \"USER: I feel anxious about tomorrow.\\nCOMPATIKA:\",\n",
    "    \"USER: I want to tell my partner I need more time together.\\nCOMPATIKA:\",\n",
    "    \"USER: I feel guilty because I blamed someone unfairly.\\nCOMPATIKA:\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Prompt:\", t)\n",
    "    response = generate_response(t)\n",
    "    print(\"Compatika:\", response)\n",
    "    print()\n",
    "\n",
    "# ============================================================\n",
    "# 5ï¸âƒ£ (Optional) Interactive Chat Mode\n",
    "# ============================================================\n",
    "while True:\n",
    "    user_input = input(\"\\nUSER: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        break\n",
    "    prompt = f\"USER: {user_input}\\nCOMPATIKA:\"\n",
    "    reply = generate_response(prompt)\n",
    "    print(\"Compatika:\", reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d60e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aman\\AppData\\Local\\Temp\\ipykernel_7736\\95136868.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"compatika_v1alpha_scratch.pt\", map_location=\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model and tokenizer loaded successfully\n",
      "Vocab size: 10000\n",
      "\n",
      "======================================================================\n",
      "Prompt: USER: I'm really upset. My friend ignored my message.\n",
      "COMPATIKA:\n",
      "Compatika: USER: I'm really upset. My friend ignored my message. COMPATIKA:\n",
      "\n",
      "======================================================================\n",
      "Prompt: USER: I feel anxious about tomorrow.\n",
      "COMPATIKA:\n",
      "Compatika: USER: I feel anxious about tomorrow. COMPATIKA: I feel anxious about\n",
      "\n",
      "======================================================================\n",
      "Prompt: USER: I want to tell my partner I need more time together.\n",
      "COMPATIKA:\n",
      "Compatika: USER: I want to tell my partner I need more time together. COMPATIKA: I\n",
      "\n",
      "======================================================================\n",
      "Prompt: USER: I feel guilty because I blamed someone unfairly.\n",
      "COMPATIKA:\n",
      "Compatika: USER: I feel guilty because I blamed someone unfairly. COMPATIKA: I feel guilty â‡  to someone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sentencepiece as spm\n",
    "\n",
    "# ============================================================\n",
    "# 1ï¸âƒ£ Model Definition (must match training setup)\n",
    "# ============================================================\n",
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        emb = emb.permute(1, 0, 2)\n",
    "        encoded = self.encoder(emb)\n",
    "        out = self.fc(encoded)\n",
    "        return out.permute(1, 0, 2)\n",
    "\n",
    "# ============================================================\n",
    "# 2ï¸âƒ£ Load Tokenizer and Model Weights\n",
    "# ============================================================\n",
    "sp = spm.SentencePieceProcessor(model_file=\"tok/compatika_sp.model\")\n",
    "vocab_size = sp.get_piece_size()\n",
    "\n",
    "model = TinyTransformer(vocab_size)\n",
    "model.load_state_dict(torch.load(\"compatika_v1alpha_scratch.pt\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Model and tokenizer loaded successfully\")\n",
    "print(f\"Vocab size: {vocab_size}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 3ï¸âƒ£ Safe Generation Function (with temperature + top-p)\n",
    "# ============================================================\n",
    "def generate_response(prompt, max_len=50, temperature=0.8, top_p=0.9):\n",
    "    tokens = sp.encode(prompt, out_type=int)\n",
    "    x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out = model(x)\n",
    "\n",
    "        # Apply temperature (controls creativity)\n",
    "        logits = out[0, -1] / temperature\n",
    "        logits = torch.clamp(logits, -50, 50)  # prevent inf or overflow\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Top-p (nucleus) sampling\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_probs[sorted_indices_to_remove] = 0\n",
    "\n",
    "        # Prevent divide-by-zero or NaN errors\n",
    "        prob_sum = sorted_probs.sum()\n",
    "        if prob_sum <= 0 or torch.isnan(prob_sum):\n",
    "            sorted_probs = F.softmax(logits, dim=-1)\n",
    "            prob_sum = sorted_probs.sum()\n",
    "\n",
    "        sorted_probs = sorted_probs / prob_sum\n",
    "\n",
    "        # Sample the next token\n",
    "        next_id = torch.multinomial(sorted_probs, 1).item()\n",
    "        tokens.append(next_id)\n",
    "\n",
    "        # Stop if EOS token (ID = 3)\n",
    "        if next_id == 3:\n",
    "            break\n",
    "\n",
    "        x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    return sp.decode(tokens)\n",
    "\n",
    "# ============================================================\n",
    "# 4ï¸âƒ£ Test on Multiple Example Prompts\n",
    "# ============================================================\n",
    "tests = [\n",
    "    \"USER: I'm really upset. My friend ignored my message.\\nCOMPATIKA:\",\n",
    "    \"USER: I feel anxious about tomorrow.\\nCOMPATIKA:\",\n",
    "    \"USER: I want to tell my partner I need more time together.\\nCOMPATIKA:\",\n",
    "    \"USER: I feel guilty because I blamed someone unfairly.\\nCOMPATIKA:\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Prompt:\", t)\n",
    "    response = generate_response(t)\n",
    "    print(\"Compatika:\", response)\n",
    "    print()\n",
    "\n",
    "# ============================================================\n",
    "# 5ï¸âƒ£ Optional â€” Interactive Chat Mode\n",
    "# ============================================================\n",
    "while True:\n",
    "    user_input = input(\"\\nUSER: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"ðŸ‘‹ Compatika: Take care of yourself today. Goodbye!\")\n",
    "        break\n",
    "    prompt = f\"USER: {user_input}\\nCOMPATIKA:\"\n",
    "    reply = generate_response(prompt)\n",
    "    print(\"Compatika:\", reply)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
